
Observation:
// Agent can't manage to learn parrying, suspect is due to a lack of information
 - Added one more observation from centerMoveTo to oppEpee2/3. 
 - Added collided collider type information in BufferSensor. 

Training Configs:
// policy is almost not converging, decrease beta.
 - Decreased beta from 0.005 to 0.001. 
// Time_horizon: This number should be large enough to capture all the important behavior within a sequence of an agent's actions.
// as increased physics timestep from 0.001 to 0.002, plus the key moment is actually the last second's action, 1 sec about 500 so 512. 
 - Reduced time_horizon back to 512. 


rerun self-play:
- remove from centerMoveTo to oppEpee2/3 observation and additional raycast observation
- don't revert collision type observation (makes sense to have it from the start)
- don't revert back to tip-to-target (so don't have to compare that much observation)
- don't revert beta and time_horizon (so don't have to compare hyperparameters)
- revert reward function. 

rerun self-play 2 [setup incorrect & halted]
- increased beta from 0.001 to 0.003 (converged too quickly?)
- decrease MaxStep by a factor of 4 + increased weight of small reward by 2 => small rewards become more significant & more desirable. 
- increase save_steps and window
    - from - 
    self_play:
      save_steps: 15000
      team_change: 75000
      swap_steps: 15000
      window: 10
      play_against_latest_model_ratio: 0.5
      initial_elo: 1200.0
    - to -
    self_play:
      save_steps: 60000     // more style as there's a longer training duration in between
      team_change: 240000   // according to ML agents docs, 4 * save_steps
      swap_steps: 24000     // according to ML agent docs, team_change / swap_num
      window: 20            // save more snapshots
      play_against_latest_model_ratio: 0.5
      initial_elo: 1200.0

rerun self-play 3:
 - decreased beta back to 0.001
 - same self-play settings as self-play 2
 - reduced hintX to left, and added pointTo actions

from self-play, rerun curriculum_one:
- modify reward function. 
- use a more progressive curriculum

rerun curriculum_two: 
- use the same curriculum as curriculum_one, but add centerMoveTo to oppEpee2/3 observation and additional raycasts
- add action here?

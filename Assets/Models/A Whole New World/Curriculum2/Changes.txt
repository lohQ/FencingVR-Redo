
Observation:
// Agent can't manage to learn parrying, suspect is due to a lack of information
 - Added one more observation from centerMoveTo to oppEpee2/3. 
 - Added collided collider type information in BufferSensor. 

Training Configs:
// policy is almost not converging, decrease beta.
 - Decreased beta from 0.005 to 0.001. 
// Time_horizon: This number should be large enough to capture all the important behavior within a sequence of an agent's actions.
// as increased physics timestep from 0.001 to 0.002, plus the key moment is actually the last second's action, 1 sec about 500 so 512. 
 - Reduced time_horizon back to 512. 


rerun self-play:
- remove from centerMoveTo to oppEpee2/3 observation and additional raycast observation
- don't revert collision type observation (makes sense to have it from the start)
- don't revert back to tip-to-target (so don't have to compare that much observation)
- don't revert beta and time_horizon (so don't have to compare hyperparameters)
- revert reward function. 

rerun self-play 2 [setup incorrect & halted]
- increased beta from 0.001 to 0.003 (converged too quickly?)
- decrease MaxStep by a factor of 4 + increased weight of small reward by 2 => small rewards become more significant & more desirable. 
- increase save_steps and window
    - from - 
    self_play:
      save_steps: 15000
      team_change: 75000
      swap_steps: 15000
      window: 10
      play_against_latest_model_ratio: 0.5
      initial_elo: 1200.0
    - to -
    self_play:
      save_steps: 60000     // more style as there's a longer training duration in between
      team_change: 240000   // according to ML agents docs, 4 * save_steps
      swap_steps: 24000     // according to ML agent docs, team_change / swap_num
      window: 20            // save more snapshots
      play_against_latest_model_ratio: 0.5
      initial_elo: 1200.0

rerun self-play 3:
 - decreased beta back to 0.001
 - same self-play settings as self-play 2
 - reduced hintX to left, and added pointTo actions

control run:
 - beta 0.003
 - self-play settings with previous self-play settings divided by 2
 - balanced reward

half-sigma run:
 - beta 0.003
 - self-play settings with save_steps increased to 100000 (0.1M), window to 30 -> sigma = 0.5
 - added time step reward (can't punish out of bound..)

half-sigma-tenth-latest-model run:
 - beta 0.003
 - self-play settings with save_steps increased to 100000 (0.1M), window to 30 -> sigma = 0.5 + latest_model_ration to 0.1
 - added time step reward (can't punish out of bound..)

half-sigma-tenth-latest-model-rerun:
 - fixed observation normalization
 - added velocity observation
 - increased raycast reward threshold from 50 to 70. 

half-sigma-tenth-latest-curriculum-run:
 - purpose: whacking all the things together in an attempt to make it work
 - with velocity observation
// - remove pointTo action
 - fix reward function
 - have curriculum changing: footwork_enabled, start_point_x_rand_coef, start_point_z_rand_coef, tip_raycast_reward_threshold.
 - Phase1: footwork disabled, start_point_x_rand_coef = start_point_z_rand_coef = 0, tip_raycast_reward_threshold = 100, timestep_reward = -1.0
 - Phase2: footwork disabled, start_point_x_rand_coef = start_point_z_rand_coef = 0.5, tip_raycast_reward_threshold = 70, timestep_reward = -1.0
 - Phase3: footwork enabled, start_point_x_rand_coef = start_point_z_rand_coef = 0.1, tip_raycast_reward_threshold = 70, timestep_reward = 1.0
 - Phase3Half: footwork enabled, start_point_x_rand_coef = start_point_z_rand_coef = 0.1, tip_raycast_reward_threshold = 50, timestep_reward = 1.0
 - PhaseFinal: footwork enabled, start_point_x_rand_coef = start_point_z_rand_coef = 1.0, tip_raycast_reward_threshold = 30, timestep_reward = 1.0

- stuck in Phase1 coz can't get enough reward. 
- 

- updated some values in FinalHandController (suppination min/max, move target z offset)
- run self-play without curriculum with 30000 save_steps and 15 window and 0.5 against latest model, timestep reward 1
- then run self-play without curriculum with 60000 save_steps and 30 window and 0.2 against latest model, timestep reward 1
- then run self-play with curriculum (time penalty reduced to -0.5, lock suppination at the start)

from self-play, rerun curriculum_one:
- modify reward function. 
- use a more progressive curriculum

rerun curriculum_two: 
- use the same curriculum as curriculum_one, but add centerMoveTo to oppEpee2/3 observation and additional raycasts
- add action here?
